<!DOCTYPE html>
<html lang="en">

<head>
	<!-- hexo-inject:begin --><!-- hexo-inject:end --><meta http-equiv="content-type" content="text/html; charset=utf-8">
	<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
	
	<!-- title -->
	
	<title>
	
		Graph convolution Neural Networks | 
	 
	Jie Feng&#39;s Blog
	</title>
	
	<!-- keywords,description -->
	
		<meta name="keywords" content="Computer vision, AI, Robotics, Data science" />
	 
		<meta name="description" content="Some record for my researches and learning life." />
	

	<!-- favicon -->
	
	<link rel="shortcut icon" href="/favicon.ico">
	
  

	
<link rel="stylesheet" href="/css/main.css">

	
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css">

	
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/styles/darcula.min.css">


	
<script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/highlight.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/npm/jquery-pjax@2.0.1/jquery.pjax.min.js"></script>

	
<script src="/js/main.js"></script>

	
		
<script src="https://cdn.jsdelivr.net/npm/leancloud-storage/dist/av-min.js"></script>

		
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>

	
	
		<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	
<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
	<!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header">
    <a id="title" href="/" class="logo">Jie Feng's Blog</a>

	<ul id="menu">
		<li class="menu-item">
			<a href="/about" class="menu-item-link">ABOUT</a>
		</li>
		
		<!-- <li class="menu-item">
			<a href="https://github.com/wujun234/uid-generator-spring-boot-starter" class="menu-item-link" target="_blank">
				UidGenerator
			</a>
		</li> -->
		<li class="menu-item">
			<a href="https://github.com/JieFeng-cse" class="menu-item-link" target="_blank">
				<i class="fa fa-github fa-2x"></i>
			</a>
		</li>
	</ul>
</header>

	
<div id="sidebar">
	<button id="sidebar-toggle" class="toggle" ><i class="fa fa-arrow-right " aria-hidden="true"></i></button>
	
	<div id="site-toc">
		<input id="search-input" class="search-input" type="text" placeholder="search...">
		<div id="tree">
			

			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										Graph Matching
									</a>
									
							<ul>
								<li class="file active">
									<a href="/2020/06/20/Graph%20Matching/Graph%20Convolution%20Neural%20Networks/">
										Graph Convolution Neural Networks
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2020/06/20/Graph%20Matching/Graph%20Similarity%20Computation/">
										Graph Similarity Computation
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2020/06/20/Graph%20Matching/Pooling/">
										Pooling
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										Tutorial
									</a>
									
							<ul>
								<li class="file">
									<a href="/2020/05/28/Tutorial/Before/">
										Before
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
		</div>
	</div>
</div>

	<!-- 引入正文 -->
	<div id="content">
		<h1 id="article-title">

	Graph Convolution Neural Networks
</h1>
<div class="article-meta">
	
	<span>Jie Feng</span>
	<span>2020-06-20 20:11:28</span>
    
		<div id="article-categories">
            
		</div>
    
</div>

<div id="article-content">
	<hr>
<h1 id="Graph-Convolution-Neural-Networks"><a href="#Graph-Convolution-Neural-Networks" class="headerlink" title="Graph Convolution Neural Networks"></a>Graph Convolution Neural Networks</h1><p>This part will introduce some very import GCN models, those convolution techniques are fundamental blocks for GNNs and I will try my best to conclude the basic ideas behind those methods. </p>
<p>Before those approaches proposed, there are some node embedding methods employ random walk statistics and matrix factorization-based learning objectives, which have close relationships to classic approaches to spectral clustering, multidimensional scaling and so on. Those methods will not be discussed there.</p>
<h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li>GCN    <a href="https://arxiv.org/pdf/1609.02907.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1609.02907.pdf</a> by kipf</li>
<li>GraphSAGE  <a href="https://arxiv.org/pdf/1706.02216.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1706.02216.pdf</a></li>
<li>GAT <a href="https://mila.quebec/wp-content/uploads/2018/07/d1ac95b60310f43bb5a0b8024522fbe08fb2a482.pdf" target="_blank" rel="noopener">https://mila.quebec/wp-content/uploads/2018/07/d1ac95b60310f43bb5a0b8024522fbe08fb2a482.pdf</a></li>
<li>GIN <a href="https://arxiv.org/pdf/1810.00826.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.00826.pdf</a></li>
<li>rGCN  <a href="https://arxiv.org/abs/1703.06103" target="_blank" rel="noopener">https://arxiv.org/abs/1703.06103</a> by kipf</li>
<li>rGAT <a href="https://arxiv.org/pdf/1904.05811.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.05811.pdf</a></li>
</ul>
<h2 id="GCN-spectral"><a href="#GCN-spectral" class="headerlink" title="GCN(spectral)"></a>GCN(spectral)</h2><p>This is one of the most prestigious works in graph neural network, serves as the most important block in many GNN frameworks.</p>
<p>To begin with, GCN is a simple layer-wise propagation rule for neural network models that operate directly on graphs. The neural network model f(X, A) is proposed to encode the graph, where X denotes the feature matrix and A is the adjacency matrix (binary or weighted). If the nodes are not labeled, empirically you can use an identity matrix as X at the beginning. </p>
<p>The propagation rule can be described as:</p>
<script type="math/tex; mode=display">
H^{l+1} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)}).</script><p>Here, $\tilde{A} = A + I_N$ is the adjacency matrix of the undirected graph G with added self-connections ($I_N$ is the identity matrix), $D_{ii} = \sum_j \tilde{A}_{ij}$ (degree matrix) and $H^{(l)}$ is the matrix of activations in the lth layer, $H^{(0)} = X$. The $\sigma(.)$ denotes an activation function such as the Relu() or others.</p>
<p>The intuition of this function is to aggregate the information of adjoining neighbors. A * H could be regarded as a node’s representation is the sum of the features of all its neighbor nodes.</p>
<p>The $\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$ can be regarded as the Laplacian matrix, which includes the degree matrix to count in the influence of the node which is being embedded and normalize $A$ as well (be definition, the sum of $A$ in a row always equals to the node’s degree, so $AD^{-1}$ could normalize A). For in-depth discussion please refer to the original paper.</p>
<h2 id="GraphSAGE-non-spectral"><a href="#GraphSAGE-non-spectral" class="headerlink" title="GraphSAGE(non-spectral)"></a>GraphSAGE(non-spectral)</h2><p>GraphSAGE, abbreviate for graph sample and aggregate, is proposed for inductive node embedding, this method learn both node’s topological features and the distribution of node features in the neighborhood simultaneously. </p>
<p>Why important?</p>
<p>The trained model can generate embeddings for unseen nodes without training again, which enables the model to be implemented in situations where the graphs may evolve. As a result, this work is the basement of PinSage, the first gcn based algorithm which is employed in the industry.</p>
<p>The idea is the work before is transductive learning methods, which means the objective of learning is generate embedding of current node directly with the whole graph in hand.  Therefore, if the graph changed, the model have to be trained again. (The real reason I thought is those models based on Laplacian eigenbasis which depends on graph structure, and that means if the structure changes, the model have to change.) However, what GraphSAGE is trying to do is to do inductive learning as they can learn from special cases to embed general nodes, in a word, to be predictive.</p>
<p>They train a set of aggregator functions that learns to aggregate feature information from a node’s local neighborhood like:</p>
<div align=center>
    <img src="/images/sage.png"/>
</div>

<p>As the illustration, GraphSAGE samples a fixed number of neighbors by random walk and aggregates level by level to accumulate local information, then transfers aggregated information  from outside into the embedded model.</p>
<p>Key: Their approach is that the model learns how to aggregate feature information from a node’s local neighborhood</p>
<h3 id="Embedding-generation"><a href="#Embedding-generation" class="headerlink" title="Embedding generation"></a>Embedding generation</h3><p>(use trained model)</p>
<div align=center>
    <img src="/images/embed.png"/>
</div>

<p>The intuition behind this is at each search depth, nodes aggregate information from their local neighbors.</p>
<p>The aggregation of the neighbor representations can be done by a variety of aggregator architectures.</p>
<p>In practice, they sample a fixed-size set of neighbors to keep the computational footprint of each batch fixed.</p>
<h3 id="Train-the-model-unsupervised"><a href="#Train-the-model-unsupervised" class="headerlink" title="Train the model (unsupervised)"></a>Train the model (unsupervised)</h3><p>Optimization technique: SGD</p>
<p>loss function:</p>
<script type="math/tex; mode=display">
J_G(z_u)=-log(\sigma(z_u^Tz_v))-QE_{v_n\sim P_{n(v)}}log(\sigma(-z_u^Tz_{v_n}))</script><p>v is a node near u on fixed-length random walk, $\sigma()$ is sigmoid function, $P_n$ is a negative sampling distribution, Q is the number of negative samples.</p>
<p>Remarkably, the $z_u$ here is generated from the features contained within a node’s neighborhood rather than training a unique embedding for each node.</p>
<p>(B.T.W, the above sentence is just copied from the original paper, I am not so clear about the meaning….)</p>
<p>This Loss function encourages the nearby nodes to have similar embeddings while the representation of disparate nodes are highly distinct.</p>
<h3 id="Aggregator-Architectures"><a href="#Aggregator-Architectures" class="headerlink" title="Aggregator Architectures"></a>Aggregator Architectures</h3><p>Mean aggregator;</p>
<p>LSTM aggregator;</p>
<p>Pooling aggregator;</p>
<h3 id="Summation"><a href="#Summation" class="headerlink" title="Summation"></a>Summation</h3><p>This could be seen as a simplification of GCN, as GCN takes global information into account, while GraphSAGE only use part local information to generate embedding, which means flexibility and robustness.</p>
<h2 id="GAT-graph-attention-network"><a href="#GAT-graph-attention-network" class="headerlink" title="GAT (graph attention network)"></a>GAT (graph attention network)</h2><p>To be different from GAN (generative adversarial network), they name this framework as GAT.</p>
<h3 id="Basic-idea"><a href="#Basic-idea" class="headerlink" title="Basic idea"></a>Basic idea</h3><p>To compute the hidden representations of each node in the graph, by attending over its neighbors, following a self-attention strategy.</p>
<p>efficient and flexible</p>
<h3 id="GAT-architecture"><a href="#GAT-architecture" class="headerlink" title="GAT architecture"></a>GAT architecture</h3><p>The input to the layer is a set of node features: $ h = \{\vec{h_1},\vec{h_2},…\vec{h_n}\}, \vec{h_i} \in R^F$ ,where N is the number of nodes, and F is the number of features in each node. The output will be a new set of features h’, and the dimension of feature vector may change.</p>
<p>Attention part:</p>
<p>The attention coefficients are calculated by:</p>
<script type="math/tex; mode=display">
e_{ij} = a(W\vec{h_i},W\vec{h_j})</script><p>Where W is a learnable weight matrix, $W \in R^{F’ \times F’}$ , i means the embedded node, this formulation indicates the importance of node j’s feature to node i, and the node j belongs to the neighborhood of node i. In the paper, they only count on first order neighbors of i (including i).</p>
<p>The attention mechanism a is a single layer feedforward neural network, parametrized by a weight vector $\vec{a} \in R^{2F’}$ with LeakyReLU applied.</p>
<script type="math/tex; mode=display">
e_{ij} = LeakyReLU(\vec{a}^T[W\vec{h}_i||W\vec{h}_j])</script><p>The || means the concatenation operation.</p>
<p>To make coefficients easily comparable, they normalize them across all choices of j using softmax function.</p>
<p>$N_i$ means the neighborhood of node i.</p>
<script type="math/tex; mode=display">
\alpha{ij} = softmax_j(e_{ij}) = \frac{exp(e_{ij})}{\sum_{k\in N_i}exp(e_{ik})}</script><p>The illustration of abovementioned process is like this:</p>
<div align=center>
    <img src="/images/gat.png"/>
</div>



<h2 id="GIN-Graph-Isomorphism-Network"><a href="#GIN-Graph-Isomorphism-Network" class="headerlink" title="GIN (Graph Isomorphism Network)"></a>GIN (Graph Isomorphism Network)</h2><p>What this ICLR paper do?</p>
<p>They prove the upper bound of GNN’s ability in distinguishing graph structures is the same level of Weisfeiler-Lehman test. And they develop the GIN which is as powerful as WL test.</p>
<p>What does WL test do?</p>
<p>To solve graph isomorphism problem which asks whether two graphs are topologically identical.</p>
<p>WL-test first aggregates the labels of nodes and their neighborhoods, and then hashes the aggregated labels into unique new labels. The algorithm decides that two graphs are non-isomorphic if at some iteration the labels of the nodes between two graphs differ.</p>
<h3 id="Theorem"><a href="#Theorem" class="headerlink" title="Theorem"></a>Theorem</h3> <div align=center>
    <img src="/images/gin.png"/>
</div>

<p>In another word, the power of GNN is determined by how well the model can map different graphs to different position while the same graphs are mapped to the same location, namely the model should be  injective.</p>
<h3 id="GIN-architecture"><a href="#GIN-architecture" class="headerlink" title="GIN architecture"></a>GIN architecture</h3>
</div>


    <div class="post-guide">
        <div class="item left">
            
        </div>
        <div class="item right">
            
              <a href="/2020/06/20/Graph%20Matching/Pooling/">
                Pooling
                <i class="fa fa-angle-right" aria-hidden="true"></i>
              </a>
            
        </div>
    </div>



	<div id="vcomments"></div>


<script>
	
		// 评论
		new Valine({
			av: AV,
			el: '#vcomments',
			notify: false,
			verify: false,
			path: window.location.pathname,
			appId: '',
			appKey: '',
			placeholder: '请输入评论',
			avatar: 'retro',
			recordIP: false
		})
	
	
</script>
	</div>
	<div id="footer">
	<p>
	©2020-<span id="footerYear"></span> 
	<a href="/">Jie Feng</a> 
	
	
		|
		<span id="busuanzi_container_site_pv">
			pv
			<span id="busuanzi_value_site_pv"></span>
		</span>
		|
		<span id="busuanzi_container_site_uv"> 
			uv
			<span id="busuanzi_value_site_uv"></span>
		</span>
	
	<br>
	Theme <a href="//github.com/wujun234/hexo-theme-tree" target="_blank">Tree</a>
	by <a href="//github.com/wujun234" target="_blank">WuJun</a>
	Powered by <a href="//hexo.io" target="_blank">Hexo</a>
	</p>
</div>
<script type="text/javascript"> 
	document.getElementById('footerYear').innerHTML = new Date().getFullYear() + '';
</script>
	<button id="totop-toggle" class="toggle"><i class="fa fa-angle-double-up" aria-hidden="true"></i></button>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>